---
title: "model5_O_NR"
output: html_document
date: "2023-08-03"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(parallel)
```



##objective function
```{r}
  #function for beta0 and beta1
obj = df
lr=lr_example

ds_matrix <- build_dataset(df, lr_example,2 ,1)

receiver <- obj$meta$cell_type[as.numeric(rownames(lr))[1]]
sender <- obj$meta$cell_type[na.omit(as.numeric(lr))[1]]

Y <- as.matrix(ds_example[,1],ncol = 1)
X <- as.matrix(ds_example[,2],ncol = 1)
X_0 <- as.matrix(rep(1, nrow(ds_example)),ncol = 1)

X<- cbind(X_0, X, X)

Dis <- as.matrix(obj$para$dist[obj$meta$cell_type == receiver, obj$meta$cell_type == receiver])
```



```{r}

model5 <- function(beta,
                   tau = 1,
                  sigma = 1,
                   phi = 1,
                   lambda1 = 0.0001,
                   lambda2 = 0.0001) {
  
       int = beta[1]
       beta_0 = beta[2]
       beta_1 = as.vector(beta[-c(1,2)])
       beta_new <- as.matrix(cbind(rep(int, length(beta_1)), rep(beta_0, length(beta_1)), beta_1))
       residuals = Y  - matrix(rowSums(X * beta_new), ncol = 1)
       
       
      # Calculate the squared sum of residuals
      L <- 1/(2*tau*ncol(Y))*tau * sum(residuals^2) 
     
      # Calculate the penalty terms
      R1 <- lambda1 * sum(abs(beta))
      
      # Calculate the penalty term R2

     weight <- exp(- phi * Dis)
     
      G <- sigma*weight
 
      
      beta_1 <-  as.matrix(beta_1)
   

      R2 = as.numeric(lambda2 /2*( t(beta_1) %*% G %*% beta_1))

      objective_value <- L + R1 + R2
      
      
      return(objective_value)
}
    



```


##objective_function
```{r}
objective_function <- function(beta,
                               Y,
                               X,
                               tau = 1,
                               sigma = 1,
                               phi = 1,
                               lambda1 = 0.001,
                               lambda2 = 0.00001) {
  obj1 <- as.matrix(beta, ncol = 1)
  int <- obj1[1]
  beta_0 <- obj1[2]
  beta_1 <- as.vector(obj1[-c(1, 2)])
  beta_new <- as.matrix(cbind(rep(int, length(beta_1)), rep(beta_0, length(beta_1)), beta_1))
  residuals <- Y - matrix(rowSums(X * beta_new), ncol = 1)

  # Calculate the squared sum of residuals
  L <- 1/tau * sum(residuals^2) 

  # Calculate the penalty terms
  R1 <- lambda1 * sum(abs(beta))

  # Calculate the penalty term R2
  Dis <- as.matrix(obj$para$dist[obj$meta$cell_type == receiver, obj$meta$cell_type == receiver])
  weight <- exp(-phi * Dis)
  G <- sigma * weight
  beta_1 <- as.matrix(beta_1)
  R2 <- as.numeric(lambda2 * (t(beta_1) %*% G %*% beta_1))

  objective_value <- L + R1 + R2
  return(objective_value)
}




```




## Optimization method
```{r}

newton_raphson <- function(Y, X, Dis, object, 
                           initial_params, 
                           tol = 1e-6, 
                           max_iter = 40) {
  
  L_values <- numeric(max_iter)
  iter <- 0
  params <- initial_params
  
  while (iter < max_iter) {
    
    grad <- numDeriv::grad(object, params)
    hessian <- numDeriv::hessian(object, params)

    params <- params - solve(hessian) %*% grad
    

    if (max(abs(grad)) < tol) {
      break
    }
    
    
    iter <- iter + 1
       int = params[1]
       beta_0 = params[2]
     beta_1 = as.vector(params[-c(1,2)])
       beta_new <- as.matrix(cbind(rep(int, length(beta_1)), rep(beta_0,length(beta_1)), beta_1))
      residuals = Y  - matrix(rowSums(X * beta_new), ncol = 1)
        R1 <- 0.001 * sum(abs(params))
       
      # Calculate the squared sum of residuals
          
          weight <- exp(-Dis)
 
      
      beta_1 <-  as.matrix(beta_1)
   

      R2 = as.numeric(0.001 /2*( t(beta_1) %*% weight %*% beta_1))
      
     
    L_values[iter + 1] <- 1/(2*ncol(Y))* sum(residuals^2) +R1+ R2
       
       
    print(L_values[iter + 1])
       
       
  }
  
  return(params) 
}


```


##speed up
```{r}


newton_raphson <- function(objective_function, 
                           initial_params, 
                           tol = 1e-6, 
                           max_iter = 30) {
  # Precompute constant calculations outside the loop
  grad_fn <- function(params) {
    grad <- numDeriv::grad(objective_function, params)
    grad
  }
  
  hess_fn <- function(params) {
    hessian <- numDeriv::hessian(objective_function, params)
    hessian
  }
  
  iter <- 0
  params <- initial_params
  
  # Use BFGS method for optimization with multicore parallelism
  num_cores <- detectCores()
  
  while (iter < max_iter) {
    # Calculate the gradient and Hessian using multicore parallelism
    grad_hess <- mclapply(list(grad_fn, hess_fn), function(fn) fn(params), mc.cores = num_cores)
    grad <- unlist(grad_hess[[1]])
    hessian <- unlist(grad_hess[[2]])
    
    # Update the parameters using the Newton-Raphson update rule
    params <- params - solve(matrix(hessian, ncol = length(params))) %*% grad
    
    
    # Check convergence
    if (max(abs(grad)) < tol) {
      break
    }
    iter <- iter + 1
  }
    return(list(params,L_values))
}




```


##test:

```{r}

initial_params <- matrix(rep(0, length(Y) + 2), ncol = 1)


result5 <- newton_raphson(Y, X,Dis, model5, initial_params)


```



##test

```{r}

initial_params <- matrix(rep(0, length(Y)  + 2), ncol = 1)
result5 <- newton_raphson(objective_function, initial_params)


```




## plot
```{r}
data_all5<- vector("list",2)
#color
data5_1 <- result5[-c(1,2)]

#location
data_1 <- as.matrix(ds_example[,c(ncol(ds_example)-1, ncol(ds_example))],ncol = ncol(ds_example)-2)
data_all5[[1]] <- data.frame(x =data_1[,1], y = data_1[,2], color = data5_1 )

#middle
data_all5[[2]] <- result5[2]

plot_result(data_all5, data_lig,data_els)




```




## plot
```{r}
# Create a vector of the points
points <- c(
  61.20746, 7.710811, 0.003168766, 0.0002008786, 0.0002000299, 0.0002003394,
  0.0002000027, 0.0002000082, 0.0002000115, 0.0002000685, 0.0002000123,
  0.0002000124, 0.0002000007, 0.0002000034, 0.0002000035, 0.0002000058,
  0.000200014, 0.0002000023, 0.0002000047, 0.0002000158, 0.0002000011,
  0.0002000075, 0.0002000032, 0.0002000136, 0.0002000082, 0.0002001208,
  0.0002000012, 0.0002000078, 0.0002000054, 0.0002000553, 0.0002000015,
  0.0002000533, 0.0002000106, 0.0002000105, 0.0002000043, 0.0002000607,
  0.0002000016, 0.0002000632, 0.0002000021, 0.0002000099, 0.0002000011
)

# Create a sequence of numbers from 1 to the length of points for the x-axis
x <- seq(1, length(points))

# Plot the points
plot(x, points, type = "l", xlab = "Iteration", ylab = "Abs sum of Gradient", main = "Convergence Curve_gradient")


values <- c(
  61.20746, 4.811116, 7.710811, 1.701559, 0.003168766,
  1.655221, 0.0002008786, 1.64824, 0.0002000299, 1.656437,
  0.0002003394, 1.661174, 0.0002000027, 1.657861, 0.0002000082,
  1.661175, 0.0002000115, 1.657864, 0.0002000685, 1.661175,
  0.0002000123, 1.657863, 0.0002000124, 1.661173, 0.0002000007,
  1.657861, 0.0002000034, 1.661174, 0.0002000035, 1.65786,
  0.0002000058, 1.661176, 0.000200014, 1.657865, 0.0002000023,
  1.661174, 0.0002000047, 1.657855, 0.0002000158, 1.661173,
  0.0002000011, 1.657861, 0.0002000075, 1.661174, 0.0002000032
)

# Create a sequence of numbers for the x-axis (assuming equal spacing)
x <- 1:length(values)

# Create a line plot
plot(x, values, type = "l", col = "blue", xlab = "Iteration", ylab = "Objective function", main = "Convergence Curve")


```


